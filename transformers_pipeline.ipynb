{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers Pipelines\n",
    "\n",
    "> The pipeline() is the easiest and fastest way to use a pretrained model for inference. \n",
    ">\n",
    "> You can use the pipeline() out-of-the-box for many tasks across different modalities\n",
    "\n",
    "### Example Pipeline - \"Summarization\"\n",
    "\n",
    "> Summarize news articles and other documents.\n",
    "\n",
    "for example, \"Summarize the following news article: ' ... '\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidhuvr/.pyenv/versions/3.11.5/envs/3.11.5-huggingface/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import pipeline() convenience function \n",
    "\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'sshleifer/distilbart-cnn-12-6',\n",
       " 'tokenizer': 'sshleifer/distilbart-cnn-12-6'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_summarizer = pipeline(\"summarization\")\n",
    "\n",
    "{\"model\": my_summarizer.model.name_or_path, \"tokenizer\": my_summarizer.tokenizer.name_or_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_model_overview = \"\"\"\n",
    "The Bart model was proposed in BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\n",
    "by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer on 29 Oct, 2019.\n",
    "\n",
    "According to the abstract,\n",
    "\n",
    "Bart uses a standard seq2seq/machine translation architecture with a bidirectional encoder (like BERT) and a left-to-right decoder (like GPT).\n",
    "The pretraining task involves randomly shuffling the order of the original sentences and a novel in-filling scheme, where spans of text are replaced with a single mask token.\n",
    "BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\n",
    "Tips:\n",
    "\n",
    "BART is a model with absolute position embeddings so itâ€™s usually advised to pad the inputs on the right rather than the left.\n",
    "\n",
    "Sequence-to-sequence model with an encoder and a decoder. Encoder is fed a corrupted version of the tokens, decoder is fed the original tokens (but has a mask to hide the future words like a regular transformers decoder). A composition of the following transformations are applied on the pretraining tasks for the encoder:\n",
    "\n",
    "mask random tokens (like in BERT)\n",
    "delete random tokens\n",
    "mask a span of k tokens with a single mask token (a span of 0 tokens is an insertion of a mask token)\n",
    "permute sentences\n",
    "rotate the document to make it start at a specific token\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': ' The Bart model was proposed in BART: Denoising '\n",
      "                  'Sequence-to-Sequence Pre-training for Natural Language '\n",
      "                  'Generation, Translation, and Comprehension . It uses a '\n",
      "                  'standard seq2seq/machine translation architecture with a '\n",
      "                  'bidirectional encoder (like BERT) The pretraining task '\n",
      "                  'involves randomly shuffling the order of the original '\n",
      "                  'sentences and a novel in-filling scheme .'}]\n"
     ]
    }
   ],
   "source": [
    "summary = my_summarizer(bart_model_overview)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do it _without_ the pipeline factory\n",
    "\n",
    "### Auto-Tokenizer\n",
    "\n",
    "Prepares input for Model\n",
    "\n",
    "> Nearly every NLP task begins with a tokenizer. A tokenizer converts your input into a format that can be processed by the model.\n",
    ">\n",
    "> https://huggingface.co/docs/transformers/v4.33.2/en/autoclass_tutorial#autotokenizer\n",
    "\n",
    "### Model: BartForConditionalGeneration\n",
    "\n",
    "https://huggingface.co/docs/transformers/v4.33.2/en/model_doc/bart#transformers.BartForConditionalGeneration\n",
    "\n",
    "Well, perhaps you understood the previous summary... ðŸ™ƒ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'sshleifer/distilbart-cnn-12-6',\n",
       " 'tokenizer': 'sshleifer/distilbart-cnn-12-6'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# create model and tokenizer from model id\n",
    "# \n",
    "# model class: https://huggingface.co/docs/transformers/v4.33.2/en/model_doc/bart#transformers.BartForConditionalGeneration\n",
    "# auto-tokenizer: \n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
    "\n",
    "{\"model\": my_summarizer.model.name_or_path, \"tokenizer\": my_summarizer.tokenizer.name_or_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
      " 'input_ids': tensor([[    0, 50118,   133,  8811,  1421,    21,  1850,    11, 30634,    35,\n",
      "          6743,   139,  3009, 47134,    12,   560,    12, 48245,  4086,  5048,\n",
      "            12, 32530,    13,  7278, 22205, 17362,     6, 41737,     6,     8,\n",
      "         10081, 43341,  1499, 50118,  1409,  1483,  3577,     6, 40290,  4134,\n",
      "         13768,     6,   234,  7243,   272, 24533,     6,  1127,  9193,  5977,\n",
      "          1222,  6320,   833, 18531,     6, 18392,  9772,   397, 10225,     6,\n",
      "           384,  2089, 20051,     6, 24602,   312,  2160,   260,  1417,     8,\n",
      "          5790,   525,  2645, 10770, 21410,    15,  1132,  1700,     6,   954,\n",
      "             4, 50118, 50118, 14693,     7,     5, 20372,     6, 50118, 50118,\n",
      "           387,  2013,  2939,    10,  2526, 48652,   176, 47762,    73, 37556,\n",
      "         19850,  9437,    19,    10,  2311, 43606,   337,  9689, 15362,    36,\n",
      "          3341,   163, 18854,    43,     8,    10,   314,    12,   560,    12,\n",
      "          4070,  5044, 15362,    36,  3341,   272, 10311,   322, 50118,   133,\n",
      "         11857, 32155,  3685,  6890, 22422, 30573,  1527,     5,   645,     9,\n",
      "             5,  1461, 11305,     8,    10,  5808,    11,    12,   506,  7491,\n",
      "          3552,     6,   147, 23645,     9,  2788,    32,  4209,    19,    10,\n",
      "           881, 11445, 19233,     4, 50118,   387, 11328,    16,  1605,  2375,\n",
      "            77,  2051, 14536,    13,  2788,  2706,    53,    67,  1364,   157,\n",
      "            13, 40494,  8558,     4,    85,  2856,     5,   819,     9,  3830,\n",
      "         11126, 38495,    19, 10451,  1058,  1915,    15, 12209,  9162,     8,\n",
      "           208, 12444,  2606,     6, 35499,    92,   194,    12,  1116,    12,\n",
      "           627,    12,  2013,   775,    15,    10,  1186,     9, 20372,  2088,\n",
      "          6054,     6,   864, 15635,     6,     8, 39186,  1938,  8558,     6,\n",
      "            19,  3077,     9,    62,     7,   231,   248,  5061,  8800,     4,\n",
      "         50118, 46070,    35, 50118, 50118,   387, 11328,    16,    10,  1421,\n",
      "            19,  7833,   737, 33183,   417,  1033,    98,    24,    17,    27,\n",
      "            29,  2333,  5578,     7, 11212,     5, 16584,    15,     5,   235,\n",
      "          1195,    87,     5,   314,     4, 50118, 50118, 48245,  4086,    12,\n",
      "           560,    12, 46665,  1421,    19,    41,  9689, 15362,     8,    10,\n",
      "          5044, 15362,     4, 14813, 15362,    16,  9789,    10, 40124,  1732,\n",
      "             9,     5, 22121,     6,  5044, 15362,    16,  9789,     5,  1461,\n",
      "         22121,    36,  4297,    34,    10, 11445,     7,  7433,     5,   499,\n",
      "          1617,   101,    10,  1675,  7891,   268,  5044, 15362,   322,    83,\n",
      "         15229,     9,     5,   511, 39604,    32,  5049,    15,     5, 11857,\n",
      "         32155,  8558,    13,     5,  9689, 15362,    35, 50118, 50118, 43776,\n",
      "          9624, 22121,    36,  3341,    11,   163, 18854,    43, 50118, 46888,\n",
      "          9624, 22121, 50118, 43776,    10,  8968,     9,   449, 22121,    19,\n",
      "            10,   881, 11445, 19233,    36,   102,  8968,     9,   321, 22121,\n",
      "            16,    41, 43576,     9,    10, 11445, 19233,    43, 50118, 43983,\n",
      "          4467, 11305, 50118, 12179,   877,     5,  3780,     7,   146,    24,\n",
      "           386,    23,    10,  2167, 19233, 50118,     2]])}\n"
     ]
    }
   ],
   "source": [
    "# create pytorch tensor of tokenized input\n",
    "inputs = tokenizer([bart_model_overview], return_tensors=\"pt\")\n",
    "\n",
    "inputs\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,     0,    20,  8811,  1421,    21,  1850,    11, 30634,    35,\n",
       "          6743,   139,  3009, 47134,    12,   560,    12, 48245,  4086,  5048,\n",
       "            12, 32530,    13,  7278, 22205, 17362,     6, 41737,     6,     8,\n",
       "         10081, 43341,  1499,   479,    85,  2856,     5,   819,     9,  3830,\n",
       "         11126, 38495,    19, 10451,  1058,  1915,    15, 12209,  9162,     8,\n",
       "           208, 12444,  2606,   479,    85, 35499,    92,   194,    12,  1116,\n",
       "            12,   627,    12,  2013,   775,    15,    10,  1186,     9, 20372,\n",
       "          2088,  6054,     6,   864, 15635,     6,     8, 39186,  1938,  8558,\n",
       "             6,    19,  3077,     9,    62,     7,   231,   248,  5061,  8800,\n",
       "           479,     2]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate Summary\n",
    "summary_ids = model.generate(inputs[\"input_ids\"])\n",
    "\n",
    "summary_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['</s><s> The Bart model was proposed in BART: Denoising Sequence-to-Sequence '\n",
      " 'Pre-training for Natural Language Generation, Translation, and '\n",
      " 'Comprehension. It matches the performance of RoBERTa with comparable '\n",
      " 'training resources on GLUE and SQuAD. It achieves new state-of-the-art '\n",
      " 'results on a range of abstractive dialogue, question answering, and '\n",
      " 'summarization tasks, with gains of up to 6 ROUGE.</s>']\n"
     ]
    }
   ],
   "source": [
    "# decode and print summary\n",
    "\n",
    "summary = tokenizer.batch_decode(summary_ids)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
